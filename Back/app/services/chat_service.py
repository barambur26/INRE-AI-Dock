"""
Chat business logic service for handling chat operations, quota management, and usage logging.
"""
import uuid
from datetime import datetime, timezone
from typing import Optional, List, Dict, Any, Tuple
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func
from sqlalchemy.orm import selectinload

from app.models import (
    User, Department, LLMConfiguration, DepartmentQuota, UsageLog
)\nfrom app.schemas.auth import UserProfile\nfrom app.schemas.chat import (\n    ChatSendRequest, ChatSendResponse, AvailableModelsResponse, ModelInfo,\n    UsageQuotaInfo, ChatStatsResponse, ChatHealthResponse\n)\nfrom app.services.llm_service import llm_service, LLMProviderError, LLMQuotaExceededError\nfrom app.core.database import get_async_session\n\n\nclass ChatServiceError(Exception):\n    \"\"\"Base exception for chat service errors.\"\"\"\n    pass\n\n\nclass NoAvailableModelsError(ChatServiceError):\n    \"\"\"Exception when no LLM models are available.\"\"\"\n    pass\n\n\nclass QuotaExceededError(ChatServiceError):\n    \"\"\"Exception when department quota is exceeded.\"\"\"\n    pass\n\n\nclass ChatService:\n    \"\"\"Service for managing chat operations and business logic.\"\"\"\n    \n    def __init__(self):\n        self.default_model_cache = None\n        self.cache_expiry = None\n    \n    async def send_message(\n        self, \n        request: ChatSendRequest, \n        user: UserProfile,\n        db: AsyncSession\n    ) -> ChatSendResponse:\n        \"\"\"\n        Send a chat message to an LLM and handle all business logic.\n        \n        Args:\n            request: Chat request with message and optional model_id\n            user: Current user information\n            db: Database session\n            \n        Returns:\n            Chat response with LLM response and usage information\n            \n        Raises:\n            NoAvailableModelsError: When no LLM models are available\n            QuotaExceededError: When department quota is exceeded\n            ChatServiceError: For other chat service errors\n        \"\"\"\n        try:\n            # 1. Select LLM model (specific or default)\n            if request.model_id:\n                llm_config = await self._get_model_by_id(db, request.model_id)\n                if not llm_config or not llm_config.enabled:\n                    raise ChatServiceError(f\"Model {request.model_id} not found or disabled\")\n            else:\n                llm_config = await self._get_default_model(db)\n            \n            # 2. Check department quota\n            await self._check_quota(db, user.department_id, llm_config.id)\n            \n            # 3. Send message to LLM\n            response_text, prompt_tokens, completion_tokens, estimated_cost = await llm_service.send_message(\n                message=request.message,\n                llm_config=llm_config\n            )\n            \n            # 4. Log usage\n            usage_log = await self._log_usage(\n                db=db,\n                user_id=user.id,\n                department_id=user.department_id,\n                llm_config_id=llm_config.id,\n                prompt_tokens=prompt_tokens,\n                completion_tokens=completion_tokens,\n                estimated_cost=estimated_cost,\n                request_details={\n                    \"message\": request.message,\n                    \"conversation_id\": str(request.conversation_id) if request.conversation_id else None,\n                    \"user_message_length\": len(request.message)\n                },\n                response_details={\n                    \"response\": response_text,\n                    \"response_length\": len(response_text),\n                    \"model_used\": llm_config.model_name\n                }\n            )\n            \n            # 5. Update department quota usage\n            await self._update_quota_usage(\n                db=db,\n                department_id=user.department_id,\n                llm_config_id=llm_config.id,\n                tokens_used=prompt_tokens + completion_tokens\n            )\n            \n            # 6. Create response\n            return ChatSendResponse(\n                response=response_text,\n                model_used=llm_config.model_name,\n                model_id=llm_config.id,\n                provider=llm_config.provider,\n                tokens_prompt=prompt_tokens,\n                tokens_completion=completion_tokens,\n                tokens_total=prompt_tokens + completion_tokens,\n                cost_estimated=estimated_cost,\n                conversation_id=request.conversation_id,\n                usage_log_id=usage_log.id\n            )\n            \n        except LLMProviderError as e:\n            raise ChatServiceError(f\"LLM Provider Error: {e}\")\n        except Exception as e:\n            raise ChatServiceError(f\"Unexpected error: {e}\")\n    \n    async def get_available_models(self, db: AsyncSession) -> AvailableModelsResponse:\n        \"\"\"Get list of available LLM models.\"\"\"\n        try:\n            # Get all enabled LLM configurations\n            result = await db.execute(\n                select(LLMConfiguration)\n                .where(LLMConfiguration.enabled == True)\n                .order_by(LLMConfiguration.created_at)\n            )\n            configurations = result.scalars().all()\n            \n            # Convert to ModelInfo objects\n            models = []\n            for config in configurations:\n                models.append({\n                    \"id\": str(config.id),\n                    \"model_name\": config.model_name,\n                    \"provider\": config.provider,\n                    \"enabled\": config.enabled,\n                    \"description\": config.config_json.get(\"description\") if config.config_json else None,\n                    \"capabilities\": config.config_json.get(\"capabilities\") if config.config_json else None\n                })\n            \n            # Get default model\n            default_model = None\n            if models:\n                default_config = configurations[0]  # First enabled model\n                default_model = {\n                    \"id\": str(default_config.id),\n                    \"model_name\": default_config.model_name,\n                    \"provider\": default_config.provider,\n                    \"enabled\": default_config.enabled\n                }\n            \n            return AvailableModelsResponse(\n                models=models,\n                default_model=default_model,\n                total_count=len(models)\n            )\n            \n        except Exception as e:\n            raise ChatServiceError(f\"Error getting available models: {e}\")\n    \n    async def get_usage_quota_info(\n        self, \n        user: UserProfile, \n        db: AsyncSession\n    ) -> UsageQuotaInfo:\n        \"\"\"Get usage quota information for user's department.\"\"\"\n        try:\n            # Get department quota for the default model\n            default_model = await self._get_default_model(db)\n            \n            result = await db.execute(\n                select(DepartmentQuota)\n                .where(\n                    DepartmentQuota.department_id == user.department_id,\n                    DepartmentQuota.llm_config_id == default_model.id\n                )\n            )\n            quota = result.scalar_one_or_none()\n            \n            if not quota:\n                # Create default quota if not exists\n                quota = DepartmentQuota(\n                    department_id=user.department_id,\n                    llm_config_id=default_model.id,\n                    monthly_limit_tokens=10000,  # Default limit\n                    current_usage_tokens=0\n                )\n                db.add(quota)\n                await db.commit()\n                await db.refresh(quota)\n            \n            # Calculate remaining tokens\n            remaining_tokens = max(0, quota.monthly_limit_tokens - quota.current_usage_tokens)\n            \n            return UsageQuotaInfo(\n                department_name=user.department_name or \"Unknown\",\n                monthly_limit=quota.monthly_limit_tokens,\n                current_usage=quota.current_usage_tokens,\n                usage_percentage=quota.usage_percentage,\n                quota_exceeded=quota.is_quota_exceeded,\n                remaining_tokens=remaining_tokens\n            )\n            \n        except Exception as e:\n            raise ChatServiceError(f\"Error getting quota info: {e}\")\n    \n    async def get_chat_stats(\n        self, \n        user: UserProfile, \n        db: AsyncSession\n    ) -> ChatStatsResponse:\n        \"\"\"Get chat statistics for the user.\"\"\"\n        try:\n            # Get user's usage logs\n            result = await db.execute(\n                select(\n                    func.count(UsageLog.id).label(\"total_messages\"),\n                    func.sum(UsageLog.tokens_prompt + UsageLog.tokens_completion).label(\"total_tokens\"),\n                    func.sum(UsageLog.cost_estimated).label(\"total_cost\"),\n                    func.avg(UsageLog.tokens_prompt + UsageLog.tokens_completion).label(\"avg_tokens\")\n                )\n                .where(UsageLog.user_id == user.id)\n            )\n            stats = result.first()\n            \n            # Get most used model\n            result = await db.execute(\n                select(\n                    LLMConfiguration.model_name,\n                    func.count(UsageLog.id).label(\"usage_count\")\n                )\n                .join(LLMConfiguration, UsageLog.llm_config_id == LLMConfiguration.id)\n                .where(UsageLog.user_id == user.id)\n                .group_by(LLMConfiguration.model_name)\n                .order_by(func.count(UsageLog.id).desc())\n                .limit(1)\n            )\n            most_used_result = result.first()\n            most_used_model = most_used_result[0] if most_used_result else None\n            \n            return ChatStatsResponse(\n                total_conversations=1,  # Placeholder - would need conversation tracking\n                total_messages=stats.total_messages or 0,\n                total_tokens_used=int(stats.total_tokens or 0),\n                total_cost=float(stats.total_cost or 0.0),\n                most_used_model=most_used_model,\n                avg_tokens_per_message=float(stats.avg_tokens or 0.0)\n            )\n            \n        except Exception as e:\n            raise ChatServiceError(f\"Error getting chat stats: {e}\")\n    \n    async def get_health_status(self, db: AsyncSession) -> ChatHealthResponse:\n        \"\"\"Get chat service health status.\"\"\"\n        try:\n            # Count total and enabled models\n            total_result = await db.execute(\n                select(func.count(LLMConfiguration.id))\n            )\n            total_models = total_result.scalar() or 0\n            \n            enabled_result = await db.execute(\n                select(func.count(LLMConfiguration.id))\n                .where(LLMConfiguration.enabled == True)\n            )\n            enabled_models = enabled_result.scalar() or 0\n            \n            # Get default model\n            default_model_name = None\n            try:\n                default_model = await self._get_default_model(db)\n                default_model_name = default_model.model_name\n            except NoAvailableModelsError:\n                pass\n            \n            status = \"healthy\" if enabled_models > 0 else \"no_models_available\"\n            quota_status = \"active\"  # Could be enhanced with actual quota checking\n            \n            return ChatHealthResponse(\n                status=status,\n                available_models=total_models,\n                enabled_models=enabled_models,\n                default_model=default_model_name,\n                quota_status=quota_status,\n                timestamp=datetime.now(timezone.utc)\n            )\n            \n        except Exception as e:\n            raise ChatServiceError(f\"Error getting health status: {e}\")\n    \n    async def _get_default_model(self, db: AsyncSession) -> LLMConfiguration:\n        \"\"\"Get the default (first enabled) LLM model.\"\"\"\n        result = await db.execute(\n            select(LLMConfiguration)\n            .where(LLMConfiguration.enabled == True)\n            .order_by(LLMConfiguration.created_at)\n            .limit(1)\n        )\n        model = result.scalar_one_or_none()\n        \n        if not model:\n            raise NoAvailableModelsError(\"No enabled LLM models available\")\n        \n        return model\n    \n    async def _get_model_by_id(self, db: AsyncSession, model_id: UUID) -> Optional[LLMConfiguration]:\n        \"\"\"Get LLM model by ID.\"\"\"\n        result = await db.execute(\n            select(LLMConfiguration)\n            .where(LLMConfiguration.id == model_id)\n        )\n        return result.scalar_one_or_none()\n    \n    async def _check_quota(\n        self, \n        db: AsyncSession, \n        department_id: UUID, \n        llm_config_id: UUID,\n        estimated_tokens: int = 1000  # Conservative estimate\n    ) -> None:\n        \"\"\"Check if department has quota available for the request.\"\"\"\n        result = await db.execute(\n            select(DepartmentQuota)\n            .where(\n                DepartmentQuota.department_id == department_id,\n                DepartmentQuota.llm_config_id == llm_config_id\n            )\n        )\n        quota = result.scalar_one_or_none()\n        \n        if not quota:\n            # Create default quota if not exists\n            quota = DepartmentQuota(\n                department_id=department_id,\n                llm_config_id=llm_config_id,\n                monthly_limit_tokens=10000,  # Default limit\n                current_usage_tokens=0\n            )\n            db.add(quota)\n            await db.commit()\n            return\n        \n        # Check if adding estimated tokens would exceed quota\n        if quota.current_usage_tokens + estimated_tokens > quota.monthly_limit_tokens:\n            raise QuotaExceededError(\n                f\"Department quota exceeded. Current usage: {quota.current_usage_tokens}, \"\n                f\"Limit: {quota.monthly_limit_tokens}, Estimated request: {estimated_tokens}\"\n            )\n    \n    async def _log_usage(\n        self,\n        db: AsyncSession,\n        user_id: UUID,\n        department_id: UUID,\n        llm_config_id: UUID,\n        prompt_tokens: int,\n        completion_tokens: int,\n        estimated_cost: float,\n        request_details: Dict[str, Any],\n        response_details: Dict[str, Any]\n    ) -> UsageLog:\n        \"\"\"Log LLM usage to the database.\"\"\"\n        usage_log = UsageLog(\n            id=uuid.uuid4(),\n            user_id=user_id,\n            department_id=department_id,\n            llm_config_id=llm_config_id,\n            timestamp=datetime.now(timezone.utc),\n            tokens_prompt=prompt_tokens,\n            tokens_completion=completion_tokens,\n            cost_estimated=estimated_cost,\n            request_details=request_details,\n            response_details=response_details\n        )\n        \n        db.add(usage_log)\n        await db.commit()\n        await db.refresh(usage_log)\n        \n        return usage_log\n    \n    async def _update_quota_usage(\n        self,\n        db: AsyncSession,\n        department_id: UUID,\n        llm_config_id: UUID,\n        tokens_used: int\n    ) -> None:\n        \"\"\"Update department quota usage.\"\"\"\n        result = await db.execute(\n            select(DepartmentQuota)\n            .where(\n                DepartmentQuota.department_id == department_id,\n                DepartmentQuota.llm_config_id == llm_config_id\n            )\n        )\n        quota = result.scalar_one_or_none()\n        \n        if quota:\n            quota.current_usage_tokens += tokens_used\n            await db.commit()\n\n\n# Global chat service instance\nchat_service = ChatService()\n